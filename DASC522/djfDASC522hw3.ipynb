{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZzLD5Tb6llS/bCI/su5vx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djfLtC0dr/python-playground/blob/main/DASC522/djfDASC522hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sqlite3 Database & Helper Classes"
      ],
      "metadata": {
        "id": "ZQBsGEy0cBFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I4jjyJULXi-c"
      },
      "outputs": [],
      "source": [
        "import sqlparse\n",
        "from sqlparse.sql import IdentifierList, Identifier,  Function\n",
        "from sqlparse.tokens import Keyword, DML\n",
        "from collections import namedtuple\n",
        "import itertools\n",
        "import sqlite3\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "\n",
        "class Reference(namedtuple('Reference', ['schema', 'name', 'alias', 'is_function'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def has_alias(self):\n",
        "        return self.alias is not None\n",
        "\n",
        "    @property\n",
        "    def is_query_alias(self):\n",
        "        return self.name is None and self.alias is not None\n",
        "\n",
        "    @property\n",
        "    def is_table_alias(self):\n",
        "        return self.name is not None and self.alias is not None and not self.is_function\n",
        "\n",
        "    @property\n",
        "    def full_name(self):\n",
        "        if self.schema is None:\n",
        "            return self.name\n",
        "        else:\n",
        "            return self.schema + '.' + self.name\n",
        "\n",
        "def _is_subselect(parsed):\n",
        "    if not parsed.is_group:\n",
        "        return False\n",
        "    for item in parsed.tokens:\n",
        "        if item.ttype is DML and item.value.upper() in ('SELECT', 'INSERT',\n",
        "                                                        'UPDATE', 'CREATE', 'DELETE'):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _identifier_is_function(identifier):\n",
        "    return any(isinstance(t, Function) for t in identifier.tokens)\n",
        "\n",
        "\n",
        "def _extract_from_part(parsed):\n",
        "    tbl_prefix_seen = False\n",
        "    for item in parsed.tokens:\n",
        "        if item.is_group:\n",
        "            for x in _extract_from_part(item):\n",
        "                yield x\n",
        "        if tbl_prefix_seen:\n",
        "            if _is_subselect(item):\n",
        "                for x in _extract_from_part(item):\n",
        "                    yield x\n",
        "            # An incomplete nested select won't be recognized correctly as a\n",
        "            # sub-select. eg: 'SELECT * FROM (SELECT id FROM user'. This causes\n",
        "            # the second FROM to trigger this elif condition resulting in a\n",
        "            # StopIteration. So we need to ignore the keyword if the keyword\n",
        "            # FROM.\n",
        "            # Also 'SELECT * FROM abc JOIN def' will trigger this elif\n",
        "            # condition. So we need to ignore the keyword JOIN and its variants\n",
        "            # INNER JOIN, FULL OUTER JOIN, etc.\n",
        "            elif item.ttype is Keyword and (\n",
        "                    not item.value.upper() == 'FROM') and (\n",
        "                    not item.value.upper().endswith('JOIN')):\n",
        "                tbl_prefix_seen = False\n",
        "            else:\n",
        "                yield item\n",
        "        elif item.ttype is Keyword or item.ttype is Keyword.DML:\n",
        "            item_val = item.value.upper()\n",
        "            if (item_val in ('COPY', 'FROM', 'INTO', 'UPDATE', 'TABLE') or\n",
        "                    item_val.endswith('JOIN')):\n",
        "                tbl_prefix_seen = True\n",
        "        # 'SELECT a, FROM abc' will detect FROM as part of the column list.\n",
        "        # So this check here is necessary.\n",
        "        elif isinstance(item, IdentifierList):\n",
        "            for identifier in item.get_identifiers():\n",
        "                if (identifier.ttype is Keyword and\n",
        "                        identifier.value.upper() == 'FROM'):\n",
        "                    tbl_prefix_seen = True\n",
        "                    break\n",
        "\n",
        "def _extract_table_identifiers(token_stream):\n",
        "    for item in token_stream:\n",
        "        if isinstance(item, IdentifierList):\n",
        "            for ident in item.get_identifiers():\n",
        "                try:\n",
        "                    alias = ident.get_alias()\n",
        "                    schema_name = ident.get_parent_name()\n",
        "                    real_name = ident.get_real_name()\n",
        "                except AttributeError:\n",
        "                    continue\n",
        "                if real_name:\n",
        "                    yield Reference(schema_name, real_name,\n",
        "                                    alias, _identifier_is_function(ident))\n",
        "        elif isinstance(item, Identifier):\n",
        "            yield Reference(item.get_parent_name(), item.get_real_name(),\n",
        "                            item.get_alias(), _identifier_is_function(item))\n",
        "        elif isinstance(item, Function):\n",
        "            yield Reference(item.get_parent_name(), item.get_real_name(),\n",
        "                            item.get_alias(), _identifier_is_function(item))\n",
        "\n",
        "def extract_tables(sql):\n",
        "    # let's handle multiple statements in one sql string\n",
        "    extracted_tables = []\n",
        "    statements = list(sqlparse.parse(sql))\n",
        "    for statement in statements:\n",
        "        stream = _extract_from_part(statement)\n",
        "        extracted_tables.append([ref.name for ref in _extract_table_identifiers(stream)])\n",
        "    return list(itertools.chain(*extracted_tables))\n",
        "\n",
        "class CSVDriver:\n",
        "    def __init__(self, table_dir_path: str):\n",
        "        self.table_dir_path = table_dir_path  # where tables (ie. csv files) are located\n",
        "        self._con = None\n",
        "\n",
        "    @property\n",
        "    def con(self) -> sqlite3.Connection:\n",
        "        \"\"\"Make a singleton connection to an in-memory SQLite database\"\"\"\n",
        "        if not self._con:\n",
        "            self._con = sqlite3.connect(\":memory:\")\n",
        "        return self._con\n",
        "    \n",
        "    def _exists(self, table: str) -> bool:\n",
        "        query = \"\"\"\n",
        "        SELECT name\n",
        "        FROM sqlite_master \n",
        "        WHERE type ='table'\n",
        "        AND name NOT LIKE 'sqlite_%';\n",
        "        \"\"\"\n",
        "        tables = self.con.execute(query).fetchall()\n",
        "        return table in tables\n",
        "\n",
        "    def _load_table_to_mem(self, table: str, sep: str = None) -> None:\n",
        "        \"\"\"\n",
        "        Load a CSV into an in-memory SQLite database\n",
        "        sep is set to None in order to force pandas to auto-detect the delimiter\n",
        "        \"\"\"\n",
        "        if self._exists(table):\n",
        "            return\n",
        "        file_name = table + \".csv\"\n",
        "        path = os.path.join(self.table_dir_path, file_name)\n",
        "        if not os.path.exists(path):\n",
        "            raise ValueError(f\"CSV table {table} does not exist in {self.table_dir_path}\")\n",
        "        df = pd.read_csv(path, sep=sep, engine=\"python\")  # set engine to python to skip pandas' warning\n",
        "        df.to_sql(table, self.con, if_exists='replace', index=False, chunksize=10000)\n",
        "\n",
        "    def query(self, query: str) -> List[tuple]:\n",
        "        \"\"\"\n",
        "        Run an SQL query on CSV file(s). \n",
        "        Tables are loaded from table_dir_path\n",
        "        \"\"\"\n",
        "        tables = extract_tables(query)\n",
        "        for table in tables:\n",
        "            self._load_table_to_mem(table)\n",
        "        cursor = self.con.cursor()\n",
        "        cursor.execute(query)\n",
        "        records = cursor.fetchall()\n",
        "        return records"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prep => Need to get Dates & Timestamps in same format"
      ],
      "metadata": {
        "id": "weI9hSni2cgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "sleep = 'sleep.csv'\n",
        "ready = 'readiness.csv'\n",
        "workout = 'workout.csv'\n",
        "mdy_format = '%m/%d/%Y'\n",
        "input_files = [sleep, ready, workout] \n",
        "\n",
        "def parse_date(input_file):\n",
        "    with open(f\"/{input_file}\", 'r') as infile, open(f\"/{input_file}_formatted.csv\", 'w') as outfile:\n",
        "      reader = csv.reader(infile)\n",
        "      headers = next(reader, None)  # returns the headers or `None` if the input is empty\n",
        "      writer = csv.writer(outfile)\n",
        "    if headers:\n",
        "      writer.writerow(headers) # write the header line\n",
        "    if input_file == sleep:\n",
        "      for row in reader:\n",
        "        dt = datetime.fromisoformat(row[1].replace('Z', '+00:00')) #parse the datetime \n",
        "        row[1] = dt.strftime(mdy_format)           #assign the revised format\n",
        "        writer.writerow(row)  \n",
        "    elif input_file == ready:\n",
        "      for row in reader:      \n",
        "        dt = datetime.strptime(row[0], '%Y-%m-%d')    #parse the datetime\n",
        "        row[0] = dt.strftime(mdy_format)     #assign the revised format\n",
        "        writer.writerow(row) \n",
        "    else: # workout\n",
        "      pass # presumably we're g2g as workout is already in mdy_format    \n",
        "\n",
        "for input_file in input_files:\n",
        "    parse_date(input_file)\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "nvXAQBsV2kYb",
        "outputId": "b04e722a-3492-4dfe-bfc2-8a18ff819566"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b5fec238e98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mparse_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b5fec238e98a>\u001b[0m in \u001b[0;36mparse_date\u001b[0;34m(input_file)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# write the header line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tables from database into Pandas dataframe object"
      ],
      "metadata": {
        "id": "balm8RUYfQ0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_path = r\"/\"\n",
        "driver = CSVDriver(db_path)\n",
        "dt_obj_sleep = datetime.strptime('sleep.timestamp', '%m/%d/%Y') # '2022-10-20T05:35:00Z' => 08/20/2022\n",
        "dt_obj_ready = datetime.strptime('readiness.date', '%m/%d/%Y') # '2022-08-20' => 08/20/2022\n",
        "dt_obj_dowrk = datetime.strptime('workout.date', '%m/%d/%Y') # '08/20/2020' => 08/20/2022\n",
        "query = \"\"\"\n",
        "SELECT S.overall_score, S.composition_score, S.revitalization_score, S.duration_score, S.deep_sleep_in_minutes, S.restlessness\n",
        "    E.First_Name, E.Last_Name, E.Gender, E.age, E.Date_of_join,  \n",
        "   P.No_of_hours_worked AS hours_worked, S.Salary_inc AS Salary_Increment\n",
        "FROM sleep S\n",
        "LEFT JOIN readiness R\n",
        "ON E.Emp_id = P.Emp_id\n",
        "LEFT JOIN workout W\n",
        "ON E.Emp_id = S.Emp_id;\n",
        "\"\"\"\n",
        "driver.query(query)"
      ],
      "metadata": {
        "id": "EfbeaeGcfaLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}