{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyNnda8nfjrDSjT4aM9pB+g+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djfLtC0dr/python-playground/blob/main/DASC522/djfDASC522hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sqlite3 Database & Helper Classes"
      ],
      "metadata": {
        "id": "ZQBsGEy0cBFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I4jjyJULXi-c"
      },
      "outputs": [],
      "source": [
        "import sqlparse\n",
        "from sqlparse.sql import IdentifierList, Identifier,  Function\n",
        "from sqlparse.tokens import Keyword, DML\n",
        "from collections import namedtuple\n",
        "import itertools\n",
        "import sqlite3\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "class Reference(namedtuple('Reference', ['schema', 'name', 'alias', 'is_function'])):\n",
        "    __slots__ = ()\n",
        "\n",
        "    def has_alias(self):\n",
        "        return self.alias is not None\n",
        "\n",
        "    @property\n",
        "    def is_query_alias(self):\n",
        "        return self.name is None and self.alias is not None\n",
        "\n",
        "    @property\n",
        "    def is_table_alias(self):\n",
        "        return self.name is not None and self.alias is not None and not self.is_function\n",
        "\n",
        "    @property\n",
        "    def full_name(self):\n",
        "        if self.schema is None:\n",
        "            return self.name\n",
        "        else:\n",
        "            return self.schema + '.' + self.name\n",
        "\n",
        "def _is_subselect(parsed):\n",
        "    if not parsed.is_group:\n",
        "        return False\n",
        "    for item in parsed.tokens:\n",
        "        if item.ttype is DML and item.value.upper() in ('SELECT', 'INSERT',\n",
        "                                                        'UPDATE', 'CREATE', 'DELETE'):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _identifier_is_function(identifier):\n",
        "    return any(isinstance(t, Function) for t in identifier.tokens)\n",
        "\n",
        "\n",
        "def _extract_from_part(parsed):\n",
        "    tbl_prefix_seen = False\n",
        "    for item in parsed.tokens:\n",
        "        if item.is_group:\n",
        "            for x in _extract_from_part(item):\n",
        "                yield x\n",
        "        if tbl_prefix_seen:\n",
        "            if _is_subselect(item):\n",
        "                for x in _extract_from_part(item):\n",
        "                    yield x\n",
        "            # An incomplete nested select won't be recognized correctly as a\n",
        "            # sub-select. eg: 'SELECT * FROM (SELECT id FROM user'. This causes\n",
        "            # the second FROM to trigger this elif condition resulting in a\n",
        "            # StopIteration. So we need to ignore the keyword if the keyword\n",
        "            # FROM.\n",
        "            # Also 'SELECT * FROM abc JOIN def' will trigger this elif\n",
        "            # condition. So we need to ignore the keyword JOIN and its variants\n",
        "            # INNER JOIN, FULL OUTER JOIN, etc.\n",
        "            elif item.ttype is Keyword and (\n",
        "                    not item.value.upper() == 'FROM') and (\n",
        "                    not item.value.upper().endswith('JOIN')):\n",
        "                tbl_prefix_seen = False\n",
        "            else:\n",
        "                yield item\n",
        "        elif item.ttype is Keyword or item.ttype is Keyword.DML:\n",
        "            item_val = item.value.upper()\n",
        "            if (item_val in ('COPY', 'FROM', 'INTO', 'UPDATE', 'TABLE') or\n",
        "                    item_val.endswith('JOIN')):\n",
        "                tbl_prefix_seen = True\n",
        "        # 'SELECT a, FROM abc' will detect FROM as part of the column list.\n",
        "        # So this check here is necessary.\n",
        "        elif isinstance(item, IdentifierList):\n",
        "            for identifier in item.get_identifiers():\n",
        "                if (identifier.ttype is Keyword and\n",
        "                        identifier.value.upper() == 'FROM'):\n",
        "                    tbl_prefix_seen = True\n",
        "                    break\n",
        "\n",
        "def _extract_table_identifiers(token_stream):\n",
        "    for item in token_stream:\n",
        "        if isinstance(item, IdentifierList):\n",
        "            for ident in item.get_identifiers():\n",
        "                try:\n",
        "                    alias = ident.get_alias()\n",
        "                    schema_name = ident.get_parent_name()\n",
        "                    real_name = ident.get_real_name()\n",
        "                except AttributeError:\n",
        "                    continue\n",
        "                if real_name:\n",
        "                    yield Reference(schema_name, real_name,\n",
        "                                    alias, _identifier_is_function(ident))\n",
        "        elif isinstance(item, Identifier):\n",
        "            yield Reference(item.get_parent_name(), item.get_real_name(),\n",
        "                            item.get_alias(), _identifier_is_function(item))\n",
        "        elif isinstance(item, Function):\n",
        "            yield Reference(item.get_parent_name(), item.get_real_name(),\n",
        "                            item.get_alias(), _identifier_is_function(item))\n",
        "\n",
        "def extract_tables(sql):\n",
        "    # let's handle multiple statements in one sql string\n",
        "    extracted_tables = []\n",
        "    statements = list(sqlparse.parse(sql))\n",
        "    for statement in statements:\n",
        "        stream = _extract_from_part(statement)\n",
        "        extracted_tables.append([ref.name for ref in _extract_table_identifiers(stream)])\n",
        "    return list(itertools.chain(*extracted_tables))\n",
        "\n",
        "class CSVDriver:\n",
        "    def __init__(self, table_dir_path: str):\n",
        "        self.table_dir_path = table_dir_path  # where tables (ie. csv files) are located\n",
        "        self._con = None\n",
        "\n",
        "    @property\n",
        "    def con(self) -> sqlite3.Connection:\n",
        "        \"\"\"Make a singleton connection to an in-memory SQLite database\"\"\"\n",
        "        if not self._con:\n",
        "            self._con = sqlite3.connect(\":memory:\")\n",
        "        return self._con\n",
        "    \n",
        "    def _exists(self, table: str) -> bool:\n",
        "        query = \"\"\"\n",
        "        SELECT name\n",
        "        FROM sqlite_master \n",
        "        WHERE type ='table'\n",
        "        AND name NOT LIKE 'sqlite_%';\n",
        "        \"\"\"\n",
        "        tables = self.con.execute(query).fetchall()\n",
        "        return table in tables\n",
        "\n",
        "    def _load_table_to_mem(self, table: str, sep: str = None) -> None:\n",
        "        \"\"\"\n",
        "        Load a CSV into an in-memory SQLite database\n",
        "        sep is set to None in order to force pandas to auto-detect the delimiter\n",
        "        \"\"\"\n",
        "        if self._exists(table):\n",
        "            return\n",
        "        file_name = table + \".csv\"\n",
        "        path = os.path.join(self.table_dir_path, file_name)\n",
        "        if not os.path.exists(path):\n",
        "            raise ValueError(f\"CSV table {table} does not exist in {self.table_dir_path}\")\n",
        "        df = pd.read_csv(path, sep=sep, engine=\"python\")  # set engine to python to skip pandas' warning\n",
        "        df.to_sql(table, self.con, if_exists='replace', index=False, chunksize=10000)\n",
        "\n",
        "    def query(self, query: str) -> List[tuple]:\n",
        "        \"\"\"\n",
        "        Run an SQL query on CSV file(s). \n",
        "        Tables are loaded from table_dir_path\n",
        "        \"\"\"\n",
        "        tables = extract_tables(query)\n",
        "        for table in tables:\n",
        "            self._load_table_to_mem(table)\n",
        "        cursor = self.con.cursor()\n",
        "        cursor.execute(query)\n",
        "        records = cursor.fetchall()\n",
        "        return records"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prep => Need to get Dates & Timestamps in same format"
      ],
      "metadata": {
        "id": "weI9hSni2cgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "sleep = 'sleep'\n",
        "ready = 'readiness'\n",
        "workout = 'workout'\n",
        "mdy_format = '%m/%d/%Y'\n",
        "input_files = [sleep, ready, workout] \n",
        "\n",
        "def parse_date(input_file):\n",
        "    with open(f\"/{input_file}.csv\", 'r') as infile, open(f\"/t_{input_file}.csv\", 'w') as outfile:\n",
        "      reader = csv.reader(infile)\n",
        "      headers = next(reader, None)  # returns the headers or `None` if the input is empty\n",
        "      writer = csv.writer(outfile)\n",
        "      if headers:\n",
        "        writer.writerow(headers) # write the header line\n",
        "      if input_file == sleep:\n",
        "        for row in reader:\n",
        "          dt = datetime.fromisoformat(row[1].replace('Z', '+00:00')) #parse the datetime \n",
        "          row[1] = dt.strftime(mdy_format)           #assign the revised format\n",
        "          writer.writerow(row)  \n",
        "      elif input_file == ready:\n",
        "        for row in reader:      \n",
        "          dt = datetime.strptime(row[0], '%Y-%m-%d')    #parse the datetime\n",
        "          row[0] = dt.strftime(mdy_format)     #assign the revised format\n",
        "          writer.writerow(row) \n",
        "      else: # workout\n",
        "        for row in reader:    \n",
        "          writer.writerow(row) # workout date is in mdy_format\n",
        "    return     \n",
        "\n",
        "for input_file in input_files:\n",
        "    parse_date(input_file)\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "nvXAQBsV2kYb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tables from database into Pandas dataframe object"
      ],
      "metadata": {
        "id": "balm8RUYfQ0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_path = r\"/\"\n",
        "driver = CSVDriver(db_path)\n",
        "sel_query = \"\"\"\n",
        "SELECT S.overall_score, S.composition_score, S.revitalization_score, S.duration_score, S.deep_sleep_in_minutes, S.restlessness,\n",
        "   R.readiness_score_value,R.srl_normalized_score,R.ff_normalized_score,R.hrv_normalized_score,  \n",
        "   W.notes AS rpe\n",
        "FROM t_sleep S\n",
        "LEFT JOIN t_readiness R\n",
        "ON S.timestamp= R.date\n",
        "LEFT JOIN t_workout W\n",
        "ON R.date = W.date;\n",
        "\"\"\"\n",
        "data = pd.DataFrame.from_records(driver.query(sel_query))\n",
        "print(data.info())"
      ],
      "metadata": {
        "id": "EfbeaeGcfaLD",
        "outputId": "f1dd5440-7332-4ff4-9f0a-9a532302e379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 517 entries, 0 to 516\n",
            "Data columns (total 11 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       517 non-null    int64  \n",
            " 1   1       517 non-null    int64  \n",
            " 2   2       517 non-null    int64  \n",
            " 3   3       517 non-null    int64  \n",
            " 4   4       517 non-null    int64  \n",
            " 5   5       517 non-null    float64\n",
            " 6   6       450 non-null    float64\n",
            " 7   7       450 non-null    float64\n",
            " 8   8       450 non-null    float64\n",
            " 9   9       450 non-null    float64\n",
            " 10  10      379 non-null    object \n",
            "dtypes: float64(5), int64(5), object(1)\n",
            "memory usage: 44.6+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Prep => we need to subset data by mask where @RPE exists"
      ],
      "metadata": {
        "id": "KO_DY3eENPGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = data[10].str.contains('@RPE', case=False, na=False)\n",
        "print(data[mask].head(50))"
      ],
      "metadata": {
        "id": "9MUn1zBbNeZk",
        "outputId": "3fbfaeb8-f254-48df-df9f-8b9b539961ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0   1   2   3    4         5      6     7      8     9   \\\n",
            "59   81  21  17  43   60  0.091130   89.0  84.0   89.0  40.0   \n",
            "61   81  21  17  43   60  0.091130   89.0  84.0   89.0  40.0   \n",
            "65   87  21  23  43   79  0.056695   97.0  94.0   97.0  52.0   \n",
            "69   77  16  16  45   66  0.085714   46.0  98.0   46.0  42.0   \n",
            "76   80  18  20  42   79  0.065265   64.0  96.0   64.0  62.0   \n",
            "77   80  18  20  42   79  0.065265   64.0  96.0   64.0  62.0   \n",
            "79   82  21  23  38   60  0.065183   68.0  86.0   68.0  68.0   \n",
            "81   86  21  18  47   82  0.073544   94.0  97.0   94.0  51.0   \n",
            "86   79  20  18  41   33  0.060976   59.0  71.0   59.0  55.0   \n",
            "87   78  19  18  41   56  0.045632   79.0  84.0   79.0  45.0   \n",
            "92   84  21  20  43   85  0.075812  100.0  83.0  100.0  39.0   \n",
            "93   84  21  20  43   85  0.075812  100.0  83.0  100.0  39.0   \n",
            "94   84  21  20  43   85  0.075812  100.0  83.0  100.0  39.0   \n",
            "99   85  21  21  43   63  0.065990   84.0  91.0   84.0  68.0   \n",
            "104  80  22  19  39   89  0.065301   60.0  79.0   60.0  54.0   \n",
            "106  80  22  19  39   89  0.065301   60.0  79.0   60.0  54.0   \n",
            "109  78  15  23  40   47  0.060226   60.0  94.0   60.0  67.0   \n",
            "111  78  15  23  40   47  0.060226   60.0  94.0   60.0  67.0   \n",
            "114  90  21  23  46   58  0.057386   56.0  95.0   56.0  43.0   \n",
            "115  90  21  23  46   58  0.057386   56.0  95.0   56.0  43.0   \n",
            "117  85  19  20  46   83  0.067093   72.0  80.0   72.0  53.0   \n",
            "119  85  19  20  46   83  0.067093   72.0  80.0   72.0  53.0   \n",
            "121  81  21  21  39   78  0.092246   93.0  64.0   93.0  39.0   \n",
            "122  81  21  21  39   78  0.092246   93.0  64.0   93.0  39.0   \n",
            "123  81  21  21  39   78  0.092246   93.0  64.0   93.0  39.0   \n",
            "126  79  21  15  43   74  0.070505   26.0  89.0   83.0  26.0   \n",
            "127  81  21  17  43   55  0.075472  100.0  91.0  100.0  33.0   \n",
            "128  81  21  17  43   55  0.075472  100.0  91.0  100.0  33.0   \n",
            "132  85  21  21  43   75  0.079172  100.0  86.0  100.0  43.0   \n",
            "133  85  21  21  43   75  0.079172  100.0  86.0  100.0  43.0   \n",
            "134  80  17  21  42   62  0.049220   85.0  91.0   85.0  41.0   \n",
            "135  80  17  21  42   62  0.049220   85.0  91.0   85.0  41.0   \n",
            "140  85  21  18  46   73  0.070613   76.0  91.0   76.0  50.0   \n",
            "145  82  20  20  42  121  0.066969   71.0  93.0   71.0  39.0   \n",
            "146  82  21  19  42   82  0.057257   66.0  93.0   66.0  45.0   \n",
            "147  79  20  17  42   61  0.097272   69.0  92.0   69.0  49.0   \n",
            "148  79  20  17  42   61  0.097272   69.0  92.0   69.0  49.0   \n",
            "153  84  21  20  43   66  0.043157   78.0  92.0   78.0  41.0   \n",
            "154  84  20  21  43   58  0.066185   88.0  92.0   88.0  54.0   \n",
            "156  84  20  21  43   58  0.066185   88.0  92.0   88.0  54.0   \n",
            "160  82  20  20  42   88  0.080408   49.0  79.0   49.0  45.0   \n",
            "162  75  22  17  36   88  0.076072   76.0  83.0   76.0  42.0   \n",
            "163  75  22  17  36   88  0.076072   76.0  83.0   76.0  42.0   \n",
            "167  90  22  21  47   93  0.077426   89.0  94.0   89.0  59.0   \n",
            "176  76  18  17  41  101  0.066148  100.0  96.0  100.0  49.0   \n",
            "190  86  22  20  44  107  0.057937   24.0  87.0   24.0  59.0   \n",
            "202  86  21  18  47   61  0.073093   45.0  99.0   45.0  55.0   \n",
            "206  82  21  18  43   62  0.058275   55.0  93.0   55.0  44.0   \n",
            "228  86  21  22  43   74  0.065947   50.0  95.0   50.0  47.0   \n",
            "234  78  21  15  42   77  0.070391   38.0  96.0   38.0  38.0   \n",
            "\n",
            "                                                    10  \n",
            "59   2.35mi 33 zone mins in 41:30 total mins @RPE2 ...  \n",
            "61   nChainz ⛓️ topset @RPE10 today 🫤 put 230# on n...  \n",
            "65                 @RPE3-4 🎯sub-10:00 #TDYWOD @ALPCRTC  \n",
            "69   no Fn rack had to get 😡 worked inw/ PC 💥🤘#NoRa...  \n",
            "76                                All reps 🧨🤘@RPE3/4/3  \n",
            "77                 buildz @18/35 KB @RPE8 for top set🤘  \n",
            "79   DNF, work was fun @130-160 bpm @RPE4 =>15 DDLz...  \n",
            "81   @RPE3 Rx calls for 53=>70# feeling beat-up fro...  \n",
            "86   5@RX + set-up/retrieval of implements =>0:45 u...  \n",
            "87   5x unbeltd reps for speed💥 @RPE3 @5RIR (#Super...  \n",
            "92   @RPE3-5 =>5x135 @ReverseBanded1+1+1+2x225 @Rev...  \n",
            "93   @RPE4/5/5 =>8UB @BW/4+4 nChainz ⛓️/5+3 nChainz ⛓️  \n",
            "94          unilateral KB/2x DBs/unilateral KB @RPE4-5  \n",
            "99   All sets raw nSleeves 5mm <90% 7mm >=90% #Unbe...  \n",
            "104  1x buildz 135/185/255/305 @raw nSleeves 7mm; b...  \n",
            "106  no Fn excuses, no blockers PM reckoning 1x bui...  \n",
            "109  All reps strictC2B @BW @RPE4-5 =>8x@BW UB /Res...  \n",
            "111  all reps 🧨no rebound based off 5RM 55-85% @RPE5🎯👍  \n",
            "114  all reps @beltless @nSleeves 5mm, steady @RPE4...  \n",
            "115  time domain same as P.CL for 6x rounds, last c...  \n",
            "117  No misses, started feeling 🤕 from hanging 3x s...  \n",
            "119  top set @RPE7 back was feeling little tender f...  \n",
            "121  2x3-ct paused buildz @135/185 Pushed to @ME fo...  \n",
            "122  3-ct paused builds @135/225 then #KillswitchEn...  \n",
            "123  deficit buildz off 45# Hi-Temps @185/225/315 t...  \n",
            "126  1.42 miles in 33:30 @RPE2 avg HR 105bpm => loa...  \n",
            "127  += @DipBelt laced 18# KB #ForcedAdaptation off...  \n",
            "128  Channeled @KnockdLoose after the humbling CJ m...  \n",
            "132  @ReverseDrag sled all sets @BW @10-sec avg, sp...  \n",
            "133  Didnt have the 2RIR protocol in me today to ch...  \n",
            "134  (8-min) Mob-up for Snatch =>10x 20\" Alt. Step-...  \n",
            "135  Felt good despite weakness (back) not @100% re...  \n",
            "140  Applying the #10%Rule all work 70-90-ish% @RPE...  \n",
            "145  Running low on ⛽ #BailoutDinnerFail had to dra...  \n",
            "146  Rode bike out/back to SOP dirt road .5mi 2.46 ...  \n",
            "147  #Ro3Kidz influence => told em was gonna put li...  \n",
            "148  5:00 Rest after BB glory dropped this 💎=> Left...  \n",
            "153  loadz on sled => Set 1 @BW 175#, 2-6 @200#Rest...  \n",
            "154  \"...To the Threshold!\" sets 1+2 @ReverseBanded...  \n",
            "156  Pre-Strength/Power mobilizer => 1x Rd Alpha @3...  \n",
            "160  All loads based on 3RM => @RawUnchained #Sleev...  \n",
            "162  Sets 1-6 15# mini-bandzSet 7 nChainz ⛓️@RPE7 🎯...  \n",
            "163  sets 1-6 @beltless3x buildz @ReverseBanded 65#...  \n",
            "167  approx 1-mi @convo pace l, Hike @25# vest @RPE...  \n",
            "176  @Beltless all reps nChainz (+=35#) @pprox 15-2...  \n",
            "190  All reps beltless in @5mm knee sleeves =>1x5 1...  \n",
            "202  GPP around house += rotary cut hills, seeded +...  \n",
            "206  1x Round of triplet, couplet was slow & did st...  \n",
            "228  += @RPE5 off 2x45+1x35 (just below knees) Save...  \n",
            "234  3x 🦍 🛒 #H🪵PO 🪓 & stacked => 65 #ZoneMins @RPE3...  \n"
          ]
        }
      ]
    }
  ]
}